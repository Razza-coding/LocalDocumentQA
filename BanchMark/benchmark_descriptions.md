| Benchmark | 測試內容 | 測試能力 | 補充說明 |
|-----------|----------|-----------|-----------|
| HellaSwag | 常識推理與句子完成能力測試。 | — | — |
| BoolQ | 二選一問題的事實判斷（是/否）能力。 | — | — |
| PIQA | 物理常識推理。 | 例如選擇合理的動作。 | — |
| SocialIQA | 社交情境理解與反應選擇能力。 | — | — |
| TriviaQA | 通識知識問答。 | 涵蓋大量專業與生活領域。 | — |
| Natural Questions | 真實用戶問題的檢索與回答能力測試。 | — | — |
| ARC-c | 難度較高的小學科學題目。 | 測試推理能力。 | — |
| ARC-e | 簡單版 ARC。 | 測試基本科學常識與邏輯。 | — |
| WinoGrande | 代名詞解析與語境推理測試。 | — | — |
| BIG-Bench Hard | 綜合性高難度推理、語言、推測題。 | — | — |
| DROP | 包含數字計算與多步驟推理的問答能力。 | — | — |
| AGIEval | 模擬 GRE/SAT 等學術考試的題型推理能力。 | — | — |
| MMLU | 多科目知識理解與推理基準（57 類領域）。 | — | — |
| MATH | 高中程度的數學推理與問題解決能力。 | — | — |
| GSM8K | 小學應用數學題。 | 測試連貫邏輯推理能力。 | — |
| GPQA | 測試專業問答與知識理解能力。 | — | — |
| MMLU (Pro) | MMLU 的高難度版本。 | 專業領域推理測試。 | — |
| MBPP | 基本 Python 程式設計題。 | 需寫出正確函式。 | — |
| HumanEval | 完整 Python 程式生成能力測試。 | — | — |
| MMLU (Pro COT) | 需要 Chain-of-Thought 推理的進階問答。 | — | — |
| MGSM | 多語言版本 GSM8K。 | 用於測試多語邏輯推理。 | — |
| Global-MMLU-Lite | MMLU 簡化版。 | 涵蓋多語言知識推理。 | — |
| Belebele | 多語閱讀理解（122 語言）與資訊抽取能力。 | — | — |
| WMT24++ (ChrF) | 翻譯任務。 | 用 ChrF 衡量語言間翻譯品質。 | — |
| FloRes | 雙向翻譯任務。 | 測試模型的語言互譯能力。 | — |
| XL-Sum | 多語新聞摘要生成。 | 考驗理解與壓縮能力。 | — |
| XQuAD (all) | 跨語言閱讀理解。 | 測試跨語語意一致性。 | — |
| COCOcap | 圖片敘述生成（圖 → 文）。 | 測試圖像語意理解與描述能力。 | — |
| DocVQA (val) | 從文件圖片中擷取資訊並回答問題（視覺文檔理解）。 | — | — |
| InfoVQA (val) | 圖中資訊（表格/數據）問答。 | 測試結構視覺推理。 | — |
| MMMU (pt) | 多領域多模態問答組合。 | 整合圖像與文字推理。 | — |
| TextVQA (val) | 圖中文字辨識後進行問答（視覺 + OCR 推理）。 | — | — |
| RealWorldQA | 生活中真實圖片場景下的視覺問答能力。 | — | — |
| ReMI | 圖像中根據提示執行任務的互動理解。 | — | — |
| AI2D | 插圖問答。 | 考驗教育圖解與箭頭標示理解。 | — |
| ChartQA | 針對圖表（如折線/長條圖）進行推理問答。 | — | — |
| ChartQA (augmented) | ChartQA 加強版。 | 提升圖表理解難度。 | — |
| VQAv2 | 視覺問答標準測試集。 | 涵蓋常見圖像 QA 問題。 | — |
| BLINK | 文字中的實體連結測試（如人名 → Wikipedia 條目）。 | — | — |
| OKVQA | 圖像常識問答。 | 需結合背景知識與視覺。 | — |
| TallyQA | 物體數量計算與回答。 | 考驗圖像計數能力。 | — |
| SpatialSense VQA | 測試圖中空間關係的理解與推理。 | — | — |
| CountBenchQA | 新設計的圖像計數與視覺理解基準。 | — | — |